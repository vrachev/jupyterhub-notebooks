{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b2184f",
   "metadata": {},
   "source": [
    "Currently Release Management uses a sys-perf comparison on the Performance Discovery plugin as: <br><br>\n",
    "\n",
    "<center>$ 50 \\% \\lt \\left ( 100 \\times {\\LARGE \\frac{y_{rc}}{y_{ga}}} \\right )  \\lt 150 \\% $ </center>\n",
    "\n",
    "where $ \\large y_{rc}$ is the measurement of the new release candidate, and $ \\large y_{ga}$ is the measurement of the last point release.\n",
    "\n",
    "The proposal is to use the new variables: <br>\n",
    "<center> <b>percent</b> = $  100 \\times \\LARGE \\left ( \\frac{y_{rc} - \\bar{y}}{\\bar{y}} \\right ) $ </center>\n",
    "<br>\n",
    "<center><b>z_score</b> = $ \\LARGE \\frac{y_{rc} - \\bar{y}}{\\sigma_y} $ </center>\n",
    "\n",
    "where $\\large \\bar{y} $ and $ \\large \\sigma_y $ refer to the mean and standard deviation since the last <b>Change Point</b>.\n",
    "\n",
    "Advantages:\n",
    "- more accurate as it uses more of the time series data\n",
    "- can tighten the current filter from $\\pm 50 \\%$\n",
    "- reduces signal to noise\n",
    "- Use different limits for iop/s and latency\n",
    "\n",
    "Disadvantages:\n",
    "- takes more time to run the analysis: \n",
    "    -  4 minutes to load all the 4.4.7/4.4.8 tasks over REST\n",
    "    -  6 minutes to run the mean/standard deviation algorithm on the analytics database (3,000 charts)\n",
    "- Not all the legacy data is available (started over a year ago, but some tests have been broken)\n",
    "    \n",
    "To Do:\n",
    "- Pick the limits - run on different branches over the next few releases\n",
    "    - abs(percent) > 25% | abs(z_score) > 2\n",
    "- Understand the new metrics\n",
    "    - system cpu user (%) - mean\n",
    "    - ss mem resident (MiB) - mean\n",
    "    - Data - disk xvde utilization (%) - mean\n",
    "    - Journal - disk xvdf utilization (%) - mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541b986-ef8d-4883-9837-ef351c4558ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from more_itertools import pairwise\n",
    "import requests\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from jupyter_datatables import init_datatables_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db38f98-75b2-47c9-bb62-03e804dd5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_user = os.getenv(\"PERF_DB_READ_USER\")\n",
    "mongo_pass = os.getenv(\"PERF_DB_READ_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93619c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of tasks to read from REST (about 20k in all)\n",
    "max_tasks = 4000\n",
    "\n",
    "# total number of tests to process for mean & std. dev.\n",
    "max_tests = 20000\n",
    "\n",
    "# of tasks per REST call\n",
    "batch = 100\n",
    "\n",
    "# two sys-perf commits we'd like to compare - use the sys-perf evergreen waterfall\n",
    "# https://evergreen.mongodb.com/waterfall/sys-perf-5.0.\n",
    "\n",
    "#build_a = \"sys_perf_5.0_6d9ec525e78465dcecadcff99cce953d380fedc8\" # 5.0.2\n",
    "#build_b = \"sys_perf_5.0_318fd9cabc59dc9651f3189b622af6e06ab6cd33\" # 5.0.1\n",
    "\n",
    "build_b = \"sys_perf_4.4_83b8bb8b6b325d8d8d3dfd2ad9f744bdad7d6ca0\" # 4.4.8\n",
    "build_b_label = '4.4.8'\n",
    "build_a = \"sys_perf_4.4_abb6b9c2bf675e9e2aeaecba05f0f8359d99e203\" # 4.4.7\n",
    "build_a_label = '4.4.7'\n",
    "\n",
    "# connection strings\n",
    "evg_base = \"https://cedar.mongodb.com/rest/v1/perf/version\"\n",
    "client = MongoClient(f\"mongodb+srv://{mongo_user}:{mongo_pass}@performancedata-g6tsc.mongodb.net/expanded_metrics?readPreference=secondary&readPreferenceTags=nodeType:ANALYTICS&readConcernLevel=local\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read the list of tasks for the 2 commits from REST\n",
    "\n",
    "print(\"Reading a\")\n",
    "\n",
    "skip = 0\n",
    "data = {\n",
    "    \"project\": [],\n",
    "    \"variant\": [],\n",
    "    \"task\": [],\n",
    "    \"test\": [],\n",
    "    \"measurement\": [],\n",
    "    \"args\": [],\n",
    "    \"execution\": [],\n",
    "    \"value\": [],\n",
    "}\n",
    "\n",
    "while True:\n",
    "    r = requests.get(f\"{evg_base}/{build_a}?skip={skip}&limit={batch}\")\n",
    "    if r.status_code == 404:\n",
    "        break\n",
    "\n",
    "    for cp in r.json():\n",
    "        if cp[\"rollups\"][\"stats\"]:\n",
    "            for rollup in cp[\"rollups\"][\"stats\"]:\n",
    "                data[\"project\"].append(cp[\"info\"][\"project\"])\n",
    "                data[\"variant\"].append(cp[\"info\"][\"variant\"])    \n",
    "                data[\"task\"].append(cp[\"info\"][\"task_name\"])    \n",
    "                data[\"test\"].append(cp[\"info\"][\"test_name\"])    \n",
    "                data[\"measurement\"].append(rollup[\"name\"])    \n",
    "                data[\"args\"].append(cp[\"info\"][\"args\"])\n",
    "                data[\"execution\"].append(cp[\"info\"][\"execution\"])\n",
    "                data[\"value\"].append(rollup[\"val\"])\n",
    "    print(skip, end='\\r')\n",
    "    skip += batch\n",
    "    if skip > max_tasks:\n",
    "        break\n",
    "        \n",
    "dfa = pd.DataFrame(data=data)\n",
    "\n",
    "print('')\n",
    "print(\"Reading b\")\n",
    "\n",
    "skip = 0\n",
    "data = {\n",
    "    \"project\": [],\n",
    "    \"variant\": [],\n",
    "    \"task\": [],\n",
    "    \"test\": [],\n",
    "    \"measurement\": [],\n",
    "    \"args\": [],\n",
    "    \"execution\": [],\n",
    "    \"value\": [],\n",
    "}\n",
    "\n",
    "while True:\n",
    "    r = requests.get(f\"{evg_base}/{build_b}?skip={skip}&limit={batch}\")\n",
    "    if r.status_code == 404:\n",
    "        break\n",
    "\n",
    "    for cp in r.json():\n",
    "        if cp[\"rollups\"][\"stats\"]:\n",
    "            for rollup in cp[\"rollups\"][\"stats\"]:\n",
    "                data[\"project\"].append(cp[\"info\"][\"project\"])\n",
    "                data[\"variant\"].append(cp[\"info\"][\"variant\"])    \n",
    "                data[\"task\"].append(cp[\"info\"][\"task_name\"])    \n",
    "                data[\"test\"].append(cp[\"info\"][\"test_name\"])    \n",
    "                data[\"measurement\"].append(rollup[\"name\"])    \n",
    "                data[\"args\"].append(cp[\"info\"][\"args\"])\n",
    "                data[\"execution\"].append(cp[\"info\"][\"execution\"])\n",
    "                data[\"value\"].append(rollup[\"val\"])\n",
    "    print(skip, end='\\r')\n",
    "    skip += batch\n",
    "    if skip > max_tasks:\n",
    "        break\n",
    "    \n",
    "print('')\n",
    "\n",
    "dfb = pd.DataFrame(data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Filter and merge the tasks from the 2 commits\n",
    "\n",
    "def filter_canaries(dframe):\n",
    "    \n",
    "    # ^((?!canary_|fio_|iperf|NetworkBandwidth|[01]_1c_avg_latency|[01]_1c_max_latency|oplog1|finishing|CleanUp|\n",
    "    # Setup|Quiesce|GennyOverhead|ShardCollection|EnableSharding|genny_canaries|nop_).*(?<!Setup|ActorFinished|\n",
    "    # ActorStarted))$\n",
    "\n",
    "    dframe_filtered = dframe[~dframe.test.str.match('CleanUp|canary|fio|iperf|NetworkBandwidth|finishing|Setup|Quiesce|GennyOverhead')]\n",
    "    dframe_filtered = dframe_filtered[~dframe_filtered.test.str.contains('ActorFinished|ActorStarted|Setup')]\n",
    "    return dframe_filtered\n",
    "\n",
    "print('dfa length = ', len(dfa),' dfb length = ', len(dfb))\n",
    "\n",
    "dfa = filter_canaries(dfa)\n",
    "dfb = filter_canaries(dfb)\n",
    "\n",
    "print('filtered ', len(dfa),' ', len(dfb))\n",
    "\n",
    "dfa[\"args\"]= dfa[\"args\"].apply(json.dumps)\n",
    "dfb[\"args\"]= dfb[\"args\"].apply(json.dumps)\n",
    "\n",
    "# merge our results together:\n",
    "comparison = dfa.merge(dfb, on=[\"project\",\"variant\",\"task\",\"test\",\"measurement\",\"args\"])\n",
    "\n",
    "print('length of merged comparison = ', len(comparison))\n",
    "\n",
    "found_ts = comparison[[\"project\",\"variant\",\"task\",\"test\",\"measurement\",\"args\"]]\n",
    "\n",
    "# We drop duplicates since there could be multiple executions for the same combination of the properties below.\n",
    "found_ts = found_ts.drop_duplicates()\n",
    "\n",
    "print('length after de-dup = ', len(found_ts))\n",
    "\n",
    "# keep the interesting metrics\n",
    "found_ts = found_ts[found_ts[\"measurement\"].isin(['AverageLatency',\n",
    "                                                  'ops_per_sec',\n",
    "                                                  'system cpu user (%) - mean',\n",
    "                                                  'ss mem resident (MiB) - mean',\n",
    "                                                  'Data - disk xvde utilization (%) - mean',\n",
    "                                                  'Journal - disk xvdf utilization (%) - mean'])]\n",
    "\n",
    "print('length after keeping interesting metrics = ', len(found_ts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Alex Costas: Algorithm to look up time series from the anaytics node in able to characterize \n",
    "# the stable region of results around build_a.\n",
    "\n",
    "def get_stable_region(commit_date, ts, cps):\n",
    "       \n",
    "    true_positive_orders = {\n",
    "        cp[\"order\"]\n",
    "        for cp in cps\n",
    "        if cp[\"triage\"][\"triage_status\"] == \"true_positive\"\n",
    "    }\n",
    "    len_ts = len(ts[\"data\"])\n",
    "    stable_region_bounds = (\n",
    "        [0]\n",
    "        + [idx for idx, datum in enumerate(ts[\"data\"]) if datum[\"order\"] in true_positive_orders]\n",
    "        + [len_ts]\n",
    "    )\n",
    "\n",
    "    start = end = 0\n",
    "\n",
    "    # if base commit before or after the entire time series, get the closest stable region\n",
    "    if commit_date < ts[\"data\"][0][\"commit_date\"]:\n",
    "        # first stable region\n",
    "        start = stable_region_bounds[0]\n",
    "        end = stable_region_bounds[1]\n",
    "\n",
    "    if commit_date > ts[\"data\"][len_ts - 1][\"commit_date\"]:\n",
    "        # last stable region\n",
    "        start = stable_region_bounds[-2]\n",
    "        end = stable_region_bounds[-1]\n",
    "\n",
    "    for start_bound, end_bound in pairwise(stable_region_bounds):\n",
    "        if (\n",
    "            ts[\"data\"][start_bound][\"commit_date\"]\n",
    "            <= commit_date\n",
    "            <= ts[\"data\"][end_bound - 1][\"commit_date\"]\n",
    "        ):\n",
    "            start = start_bound\n",
    "            end = end_bound\n",
    "    return [datum[\"value\"] for datum in ts[\"data\"][start:end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate the means and std dev for the Zscores\n",
    "# Must be on VPN to read the analytics DB\n",
    "print('')\n",
    "# limit number of tests\n",
    "found_ts = found_ts[0:max_tests]\n",
    "\n",
    "total = len(found_ts)\n",
    "\n",
    "stable_mean = []\n",
    "stable_std = []\n",
    "stable_length = []\n",
    "\n",
    "date_a = client[\"expanded_metrics\"][\"versions\"].find_one({\"version_id\": build_a})[\"commit_date\"]\n",
    "date_b = client[\"expanded_metrics\"][\"versions\"].find_one({\"version_id\": build_b})[\"commit_date\"]\n",
    "\n",
    "for index, row in found_ts.iterrows():\n",
    "    # some tests do not have threads.\n",
    "    if row[\"args\"] == \"null\":\n",
    "            row[\"args\"] = \"{}\"\n",
    "    ts = client[\"expanded_metrics\"][\"time_series\"].find_one({\n",
    "            \"project\": row[\"project\"],\n",
    "            \"variant\": row[\"variant\"],\n",
    "            \"task\": row[\"task\"],\n",
    "            \"test\": row[\"test\"],\n",
    "            \"args\": json.loads(row[\"args\"]),\n",
    "            \"measurement\": row[\"measurement\"],\n",
    "        })\n",
    "    cps = list(client[\"expanded_metrics\"][\"change_points\"].find({\n",
    "            \"time_series_info.project\": row[\"project\"],\n",
    "            \"time_series_info.variant\": row[\"variant\"],\n",
    "            \"time_series_info.task\": row[\"task\"],\n",
    "            \"time_series_info.test\": row[\"test\"],\n",
    "            \"time_series_info.args\": json.loads(row[\"args\"]),\n",
    "            \"time_series_info.measurement\": row[\"measurement\"],\n",
    "    }))\n",
    "    \n",
    "    try:\n",
    "      stable_region = get_stable_region(date_a, ts, cps)\n",
    "      stable_mean.append(np.mean(stable_region))\n",
    "      stable_std.append(np.std(stable_region))\n",
    "      stable_length.append(len(stable_region))\n",
    "    except:\n",
    "        # no stable region found\n",
    "        print('')\n",
    "        print('no stable region found for ', len(stable_length))\n",
    "        print('')\n",
    "        stable_mean.append(np.nan)\n",
    "        stable_std.append(np.nan)\n",
    "        stable_length.append(0)\n",
    "        pass\n",
    "    \n",
    "    print('{}/{}'.format(len(stable_length), total), end='\\r')\n",
    "\n",
    "print('')\n",
    "found_ts.insert(0, \"stable_mean\", stable_mean)\n",
    "found_ts.insert(1, \"stable_std\", stable_std)\n",
    "found_ts.insert(2, \"stable_length\", stable_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the results together:\n",
    "comparison = comparison.merge(found_ts, on=[\"project\",\"variant\",\"task\",\"test\",\"measurement\",\"args\"])\n",
    "\n",
    "#comparison[\"difference\"] = comparison[\"value_y\"] - comparison[\"value_x\"]\n",
    "#comparison[\"percentage_change\"] = ((comparison[\"value_y\"] / comparison[\"value_x\"]) * 100) - 100\n",
    "#comparison[\"difference_from_stable_mean\"] = comparison[\"value_y\"] - comparison[\"stable_mean\"]\n",
    "comparison[\"percent\"] = ((comparison[\"value_y\"] / (1.E-3+comparison[\"stable_mean\"])) * 100) - 100\n",
    "comparison[\"z_score\"] = (comparison[\"value_y\"] - comparison[\"stable_mean\"]) / (1.E-3+comparison[\"stable_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to CSV\n",
    "with open(\"compare.csv\", \"w\") as csv:\n",
    "    comparison.to_csv(csv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram the Zscores\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,14)\n",
    "\n",
    "comparison[\"z_score\"].hist(by=comparison[\"measurement\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram the % changes\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,14)\n",
    "\n",
    "comparison[\"percent\"].hist(by=comparison[\"measurement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb125d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots \n",
    "\n",
    "# %matplotlib widget\n",
    "# %matplotlib ipympl\n",
    "\n",
    "# %matplotlib inline\n",
    "# loses Engineering format\n",
    "# import mpld3\n",
    "# mpld3.enable_notebook()\n",
    "from matplotlib.ticker import EngFormatter\n",
    "\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (12, 8),\n",
    "         'axes.labelsize': 16,\n",
    "         'axes.titlesize': 16,\n",
    "         'xtick.labelsize':14,\n",
    "         'ytick.labelsize':14\n",
    "         }\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "fig, axs = plt.subplots(3,2, figsize=(12,12))\n",
    "fig.subplots_adjust(hspace = .5, wspace=.5)\n",
    "\n",
    "axs = axs.ravel()\n",
    "i=0\n",
    "for t in ['AverageLatency',\n",
    "'ops_per_sec',\n",
    "'system cpu user (%) - mean',\n",
    "'ss mem resident (MiB) - mean',\n",
    "'Data - disk xvde utilization (%) - mean',\n",
    "'Journal - disk xvdf utilization (%) - mean']:\n",
    "    axs[i].yaxis.set_major_formatter(EngFormatter()) \n",
    "    axs[i].set_title(t)\n",
    "    axs[i].set(xlabel=\"percent\", ylabel=\"z_score\")\n",
    "    axs[i].scatter(comparison[\"percent\"][(comparison[\"measurement\"] == t)],\n",
    "             comparison[\"z_score\"][(comparison[\"measurement\"] == t)], s=5)\n",
    "    i=i+1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(row_num):\n",
    "    \n",
    "    # put chart on a new pop-up    \n",
    "    from IPython import get_ipython\n",
    "    # %matplotlib widget\n",
    "    %matplotlib qt\n",
    "\n",
    "    project = comparison.loc[row_num, 'project']\n",
    "    variant = comparison.loc[row_num, 'variant']\n",
    "    task = comparison.loc[row_num, 'task']\n",
    "    test = comparison.loc[row_num, 'test']\n",
    "    measurement = comparison.loc[row_num, 'measurement']    \n",
    "    args = comparison.loc[row_num, 'args']\n",
    "    value_x = comparison.loc[row_num, 'value_x']\n",
    "    value_y = comparison.loc[row_num, 'value_y']\n",
    "    z_score = comparison.loc[row_num, 'z_score']\n",
    "    percent = comparison.loc[row_num, 'percent']\n",
    "    stable_mean = comparison.loc[row_num, 'stable_mean']\n",
    "    stable_std = comparison.loc[row_num, 'stable_std']\n",
    "\n",
    "    time_series = client[\"expanded_metrics\"][\"time_series\"].find_one(\n",
    "        { \"project\": project,\n",
    "          \"variant\": variant, \n",
    "          \"test\": test, \n",
    "          \"task\": task, \n",
    "          \"measurement\": measurement,\n",
    "         \"args\": json.loads(args)\n",
    "        }\n",
    "    )\n",
    "        \n",
    "    dates = [time_series_point[\"commit_date\"] for time_series_point in time_series[\"data\"]]\n",
    "    values = [time_series_point[\"value\"] for time_series_point in time_series[\"data\"]]\n",
    "\n",
    "    params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (16, 6),\n",
    "         'axes.labelsize': 24,\n",
    "         'axes.titlesize': 24,\n",
    "         'xtick.labelsize':10,\n",
    "         'ytick.labelsize':18}\n",
    "    plt.rcParams.update(params)\n",
    "\n",
    "    plt.suptitle(variant+' '+task+' '+test, fontsize=16)\n",
    "    plt.title(\"z_score = {:.2f}\".format(z_score)+\"  percent = {:.2f}\".format(percent), fontsize=10, loc='left')\n",
    "    plt.plot(dates, values)\n",
    "    \n",
    "    plt.xlabel(\"Commit Date\")\n",
    "    plt.ylabel(time_series[\"measurement\"])\n",
    "    \n",
    "    # add marks for the commits\n",
    "    plt.axvline(date_a, color=\"green\", linestyle=\"dotted\")\n",
    "    plt.text(date_a, value_x, build_a_label, rotation=90, fontsize=20)\n",
    "    plt.axhline(value_x, color=\"green\", linestyle=\"dotted\" )\n",
    "    plt.axvline(date_b, color=\"red\", linestyle=\"dashed\")\n",
    "    plt.text(date_b, value_y, build_b_label, rotation=90, fontsize=20)\n",
    "    plt.axhline(value_y, color=\"red\", linestyle=\"dashed\" )\n",
    "    plt.axhline(stable_mean, color=\"purple\", linestyle=\"dashdot\" )\n",
    "    plt.axhspan(stable_mean-stable_std, stable_mean+stable_std, facecolor=\"purple\", alpha=0.05)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76dddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the table as a qgrid\n",
    "\n",
    "# increase size of output window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 48em; }</style>\"))\n",
    "\n",
    "comparison = comparison.sort_values(by=['z_score', 'percent'], ignore_index=True)\n",
    "\n",
    "df = pd.DataFrame(comparison)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth', 20)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# qgrid floating format\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# add filter here to remove ok looking z_score & percentage differences\n",
    "\n",
    "ddf = df[[ 'variant', 'task', 'test', 'measurement',  'z_score', 'percent', \n",
    "          'value_x', 'value_y', 'stable_mean', 'stable_length', 'stable_std', 'args']]\n",
    "\n",
    "# save to disk\n",
    "with open(f\"selected_tasks_{build_a_label}_{build_b_label}.csv\", \"w\") as csv:\n",
    "    ddf.to_csv(csv)\n",
    "    \n",
    "import ipydatagrid\n",
    "\n",
    "info_grid = ipydatagrid.DataGrid(ddf)\n",
    "\n",
    "# display plot when row is selected\n",
    "def on_row_selected(change):    \n",
    "    plot_timeseries(change.new[0])\n",
    "    \n",
    "info_grid.observe(on_row_selected, names=['_selected_rows'])\n",
    "\n",
    "print('Click on a row to see the time-series')\n",
    "info_grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "require": {
   "paths": {
    "buttons.colvis": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.colVis.min",
    "buttons.flash": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.flash.min",
    "buttons.html5": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.html5.min",
    "buttons.print": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.print.min",
    "chartjs": "https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.8.0/Chart",
    "d3": "https://d3js.org/d3.v5.min",
    "d3-array": "https://d3js.org/d3-array.v2.min",
    "datatables.net": "https://cdn.datatables.net/1.10.18/js/jquery.dataTables",
    "datatables.net-buttons": "https://cdn.datatables.net/buttons/1.5.6/js/dataTables.buttons.min",
    "datatables.responsive": "https://cdn.datatables.net/responsive/2.2.2/js/dataTables.responsive.min",
    "datatables.scroller": "https://cdn.datatables.net/scroller/2.0.0/js/dataTables.scroller.min",
    "datatables.select": "https://cdn.datatables.net/select/1.3.0/js/dataTables.select.min",
    "jszip": "https://cdnjs.cloudflare.com/ajax/libs/jszip/2.5.0/jszip.min",
    "moment": "https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.8.0/moment",
    "pdfmake": "https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.36/pdfmake.min",
    "vfsfonts": "https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.36/vfs_fonts"
   },
   "shim": {
    "buttons.colvis": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.flash": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.html5": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.print": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "chartjs": {
     "deps": [
      "moment"
     ]
    },
    "datatables.net": {
     "exports": "$.fn.dataTable"
    },
    "datatables.net-buttons": {
     "deps": [
      "datatables.net"
     ]
    },
    "pdfmake": {
     "deps": [
      "datatables.net"
     ]
    },
    "vfsfonts": {
     "deps": [
      "datatables.net"
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
